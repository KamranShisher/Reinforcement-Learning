{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q_learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpfRTl7yKwk8",
        "colab_type": "text"
      },
      "source": [
        "# Creating Taxi Environment from OpenAI gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihw5wL46KlAv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "1051606e-f9a5-4a5a-cd0b-f177ef583359"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "state = env.reset()\n",
        "print(state)\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "121\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| :\u001b[43m \u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArjVKSsyK6Da",
        "colab_type": "text"
      },
      "source": [
        "# Possible States and actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxOTvY-KLAvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnnyjvW5LMax",
        "colab_type": "text"
      },
      "source": [
        "# Choosing Action Randomly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwKScao9LCwm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a716390-31f0-4711-887c-1d70a197f806"
      },
      "source": [
        "state = env.reset()\n",
        "counter = 0\n",
        "g = 0\n",
        "reward = None\n",
        "while reward != 20:\n",
        "    state, reward, done, info = env.step(env.action_space.sample())\n",
        "    counter += 1\n",
        "    g += reward\n",
        "print(\"Solved in {} Steps with a total reward of {}\".format(counter,g))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Solved in 4807 Steps with a total reward of -18844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X7_Xys1LvnP",
        "colab_type": "text"
      },
      "source": [
        "# Optimal Q Value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBCJIslyLSMg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "5d4111bc-2427-466a-a031-4e2774e944af"
      },
      "source": [
        "Q = np.zeros([n_states, n_actions])\n",
        "Q.shape\n",
        "# This multidimensional array will keep a history of our Q-Values for all states\n",
        "Q_hist = np.zeros([n_states, n_actions, 0])\n",
        "Q_hist.shape\n",
        "alpha = 0.618\n",
        "G = 0\n",
        "episodes = 1000\n",
        "Q = np.zeros([n_states, n_actions])\n",
        "rewardTracker = []\n",
        "\n",
        "\n",
        "for episode in range(1,episodes+1):\n",
        "    done = False\n",
        "    G, reward = 0,0\n",
        "    state = env.reset()\n",
        "    while done != True:\n",
        "        action = np.argmax(Q[state]) \n",
        "        state2, reward, done, info = env.step(action) \n",
        "        Q[state,action] += alpha * ((reward + (np.max(Q[state2]))  - Q[state,action]))\n",
        "        G += reward\n",
        "        state = state2\n",
        "    rewardTracker.append(G)\n",
        "    \n",
        "    if episode % 100 == 0:\n",
        "        print('Episode {} Reward: {}   Total Average Reward: {} '.format(episode,G, sum(rewardTracker)/len(rewardTracker)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 100 Reward: -282   Total Average Reward: -217.05 \n",
            "Episode 200 Reward: -92   Total Average Reward: -139.52 \n",
            "Episode 300 Reward: 10   Total Average Reward: -93.81333333333333 \n",
            "Episode 400 Reward: 10   Total Average Reward: -68.9 \n",
            "Episode 500 Reward: 4   Total Average Reward: -53.828 \n",
            "Episode 600 Reward: 7   Total Average Reward: -43.526666666666664 \n",
            "Episode 700 Reward: 1   Total Average Reward: -36.28142857142857 \n",
            "Episode 800 Reward: 13   Total Average Reward: -30.76375 \n",
            "Episode 900 Reward: 7   Total Average Reward: -26.467777777777776 \n",
            "Episode 1000 Reward: 2   Total Average Reward: -23.071 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-DlrJJLL-gd",
        "colab_type": "text"
      },
      "source": [
        "# Implement Optimal Q value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azfXCFfzL7k8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dcdbb2ae-ce5f-4ff4-8ad7-6bc9f0c3c2a7"
      },
      "source": [
        "state = env.reset()\n",
        "done = None\n",
        "\n",
        "while done != True:\n",
        "    # We simply take the action with the highest Q Value\n",
        "    action = np.argmax(Q[state])\n",
        "    state, reward, done, info = env.step(action)\n",
        "    env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[42mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[42m_\u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}